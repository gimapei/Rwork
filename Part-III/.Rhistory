clear()
help
age <- 35
name <- '홍길동'
age; name
age2 <- c(35,23,55)
name2 <- c('홍길동','이순신','강감')
age2;name2
age2[1]
name2[3]
name2 <- c('홍길동','이순신','강감찬')
name2[3]
var <= 1
name2[var]
name2[var]
var <= 1
var111 <= 1
var111 <- 1
name2[var111]
ls()
ls()
ls
ls
ls
ls()
num <- 10
string <- '이순'
string <- '이순신'
boolean <- TRUE
mode(num)
mode(string)
mode(boolean)
is.numeric(num)
is.character(num)
is.double(string)
mode(num)
class(num)
class(name2)
no <- c(1:5)
name <- c('a1','a2','a3','a4','a5')
no; name
emp <- data.frame(NO=no,NAME=name)
emp
mode(emp)
class(emp)
sum(10,20,30)
sum(5,4,3,2,NA)
sum(5,4,3,2,NA)
sum(5,4,3,2,NA, na.rm = T)
x <- c(1,2,3)
x <- c('1','2','3')
result <- x * 3
result <- x * 3  # error
xx <- as.numeric(x)
result <= xx * 3
result <- xx * 3
result <- xx * 3
result <- xx * 3
result
# 6) factor형 : 동일한 값을 범주로 갖는 Vector(하나의 변수에 1개 이상의 값을 가진변수, Array같은거)
gender <- c('m','f','m','m','f')
gender
man <- as.factor(gender)
man
plot(gender)
plot(gender) # error
man
plot(gender)
fgender <- as.factor(gender)
fgender
plot(fgender)
str(fgender)
class(fgender)
mode(fgender)
str(fgender)
fgender
Ogender <- factor(gender, levels=c('f','m'), labels=c('woman','man'))
Ogender
plot(Ogenger)
Ogender
Ogender <- factor(gender, levels=c('f','m'), labels=c('woman','man'))
plot(Ogenger)
plot(Ogender)
install.packages('stringr')
search()
library(string) # 패키지를 메모리에 올리는 명
library(stringr) # 패키지를 메모리에 올리는 명
search()
str_extract_all('홍길동34이순신35','[0-9]{2}')
str_extract_all('홍길동34이순신35','[a-zA-Z]{3}') #
str_extract_all('홍길동34이순신35','[가-히]{3}') #
str_extract_all('홍길동34이순신35','[가-히]{3}') # [1] "홍길동" "이순신"
data()
data(EuStockMarkets)
EuStockMarkets
EuStockMarkets[1:20]
str(EuStockMarkets)
hist(EuStockMarkets)
?sum
help(sum)
help(mean)  # sum(..., na.rm = FALSE)
mean(10,20,30)
x <- c(10,20,30)
mean(x)
getwd()
setwd('C:/Rwork/Part-I')
getwd() # [1] "C:/Rwork"
setwd('C:/Rwork')
filedata <- read.csv('c:/Rwork/Part-I/test.csv', header=T)
filedata
filedata <- read.csv('c:/Rwork/Part-I/test.csv', header=T) # 기본폴더에서 가져올때는 그냥 파일명만 명시해도 된다.
filedata
getwd()
setwd("c:/Rwork/Part-III")
data <- read.csv("three_sample.csv", header=TRUE)
data
method <- data$method
survey<- data$survey
method
survey
# 3.기술통계량(빈도분석)
table(method, useNA="ifany") # 50 50 50 -> 3그룹 모두 관찰치 50개
table(method, survey, useNA="ifany") # 그룹별 클릭수 : 1-43, 2-34, 3-37
# 4. 세집단 비율차이 검정
# prop.test(그룹별 빈도, 그룹수) -> 집단이 늘어나도 동일한 함수 사용-땡큐
prop.test(c(34,37,39), c(50,50,50)) # p-value = 0.1165 -> 귀무가설 채택
data <- read.csv("c:/Rwork/Part-III/three_sample.csv", header=TRUE)
# 2. 데이터 정제/전처리 - NA, outline 제거
data <- subset(data, !is.na(score), c(method, score))
data # method, score
# (1) 차트이용 - ontlier 보기(데이터 분포 현황 분석)
plot(data$score) # 차트로 outlier 확인 : 50이상과 음수값
barplot(data$score) # 바 차트
mean(data$score) # 14.45
# (2) outlier 제거 - 평균(14) 이상 제거
length(data$score)#91
data2 <- subset(data, score <= 14) # 14이상 제거
length(data2$score) #88(3개 제거)
# (3) 정제된 데이터 보기
x <- data2$score
boxplot(x)
plot(x)
data2$method2[data2$method==1] <- "방법1"
data2$method2[data2$method==2] <- "방법2"
data2$method2[data2$method==3] <- "방법3"
table(data2$method2) # 교육방법 별 빈도수
# 4. 동질성 검정 - 정규성 검정
# bartlett.test(종속변수 ~ 독립변수) # 독립변수(세 집단)
bartlett.test(score ~ method2, data=data2)
# 귀무가설 : 세 집단 간 분포의 모양이 동질적이다.
result <- aov(score ~ method2, data=data2)
names(result)
# aov()의 결과값은 summary()함수를 사용해야 p-value 확인
summary(result)
TukeyHSD(result)
plot(TukeyHSD(result))
result <- read.csv("C:/Rwork/Part-IV/product.csv", header=TRUE)
head(result) # 친밀도 적절성 만족도(등간척도 - 5점 척도)
summary(result) # 요약통계량
sd(result$친밀도);sd(result$적절성);sd(result$만족도)
cor(result$친밀도, result$적절성) # 0.4992086 -> 다소 높은 양의 상관관계
cor(result$친밀도, result$만족도) # 0.467145 -> 다소 높은 양의 상관관계
cor(result, method="pearson") # 피어슨 상관계수 - default
install.packages("corrgram")
library(corrgram)
corrgram(result) # 색상 적용 - 동일 색상으로 그룹화 표시
corrgram(result) # 색상 적용 - 동일 색상으로 그룹화 표시
corrgram(result, upper.panel=panel.conf) # 수치(상관계수) 추가(위쪽)
corrgram(result, lower.panel=panel.conf) # 수치(상관계수) 추가(아래쪽)
corrgram(result) # 색상 적용 - 동일 색상으로 그룹화 표시
corrgram(result, upper.panel=panel.conf) # 수치(상관계수) 추가(위쪽)
corrgram(result, lower.panel=panel.conf) # 수치(상관계수) 추가(아래쪽)
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
# 상관성,p값(*),정규분포 시각화 - 모수 검정 조건
chart.Correlation(result, histogram=, pch="+")
# spearman : 서열척도 대상 상관계수
cor(result, method="spearman")
str(result) # 'data.frame':  264 obs. of  3 variables:
y = result$만족도 # 종속변수
x = result$적절성 # 독립변수
result.lm <- lm(formula=y ~ x, data=result)
result.lm
result.lm  # 절편, 기울기
head(x)
y = 0.7789 + 0.7393 * 4
y
summary(result.lm)
plot(formula=y ~ x, data=result) # x,y 산점도 그리기
result.lm <- lm(formula=y ~ x, data=result) # 회귀모형 생성
summary(result.lm)
plot(formula=y ~ x, data=result) # x,y 산점도 그리기
str(result) # 'data.frame':  264 obs. of  3 variables:
y = result$만족도 # 종속변수
x = result$적절성 # 독립변수
result.lm <- lm(formula=y ~ x, data=result)
result.lm  # 절편, 기울기
summary(result.lm)
# (3) 단순선형회귀 시각화
plot(formula=y ~ x, data=result) # x,y 산점도 그리기
result.lm <- lm(formula=y ~ x, data=result) # 회귀모형 생성
abline(result.lm) # 회귀선 추가
y <- result$만족도 # 종속변수
x1 <- result$적절성 # 독립변수
x2 <- result$친밀도 # 독립변수
# (2) 회귀모델 생성
result.lm <- lm(formula=y ~ x1 + x2, data=result)
result.lm # 계수(절편과 기울기) 보기
# 회귀방정식 : Y(종속변수) = 절편 + 기울기1.x1 + 기울기2.x2...
# (3) 회귀모델 결과 보기
summary(result.lm) # 회귀모델 결과 보기
########################################
###  학습데이터와 검정데이터 분석
########################################
# 단계1 : 7:3비율 데이터 샘플링
result <- read.csv("C:/Rwork/Part-IV/product.csv", header=TRUE)
result
t <- sample(1:nrow(result), 0.7*nrow(result))
t
# 단계2 : 학습데이터와 검정데이터 생성
train <- result[t,] # result중 70%
dim(train) # [1] 184   3
train # 학습데이터
test <- result[-t, ] # result중 나머지 30%
dim(test) # [1] 80  3
test # 검정 데이터
# 단계3 : 회귀모델 생성 :  학습 데이터 훈련
model <- lm(formula=만족도 ~ 적절성 + 친밀도, data=train)
model # 계수보기
summary(model)
# 단계4 : 회귀모델 평가 : 검정 데이터 이용 예측기 생성
p<- predict(model, test)
# 상관계수로 성능 평가
cor(p, test$만족도)
# ----------------------------------------------------------------
#<연습문제1>  다중회귀분석 수행
# ----------------------------------------------------------------
install.packages("car")
library(car)
fit <- lm(formula=Sepal.Length ~ Sepal.Width+Petal.Length+Petal.Width, data=train)
vif(fit)
sqrt(vif(fit))>2 # root(VIF)가 2 이상인 것은 다중공선성 문제 의심
# (2) iris 변수 간의 상관계수 구하기
cor(iris[,-5]) # 변수간의 상관계수 보기(Species 제외)
#x변수 들끼 계수값이 높을 수도 있다. -> 해당 변수 제거(모형 수정) <- Petal.Width
# (3) 학습데이터와 검정데이터 분류
x <- sample(1:nrow(iris), 0.7*nrow(iris)) # 전체중 70%만 추출
train <- iris[x, ] # 학습데이터 추출
test <- iris[-x, ] # 검정데이터 추출
# (4) 회귀모델 생성 : Petal.Width 변수를 제거한 후 모델 생성
model_iris2 <- lm(formula=Sepal.Length ~ Sepal.Width+Petal.Length, data=train)
model_iris2 # 절편, 기울기(만약 기울기가 음수이면 음(-)의 영향)
# 회귀모델 결과 보기
summary(model_iris2)
# 5) 회귀모델 성능 평가
# 형식) predict(model, data) data에 y변수가 포함되어 있어야 함
p2 <- predict(model_iris2, test)# test에서 y변수를 찾아서 수치 예측
p2 # test 대상 y값 예측 결과
# 요약통계량으로 비교
summary(p2)
summary(test$Sepal.Length)
# 상관계수로 비교
cor(p2, test$Sepal.Length) #  0.8805379
# ----------------------------------------------------------------
#<연습문제2>  회귀방정식 적용 상품 구매 금액 예측
# ----------------------------------------------------------------
# part패키지 설치
install.packages("party")
library(party) # ctree() 제공
data(iris)
summary(iris)
data(iris)
summary(iris)
#----------------<<실습1>>-------------------------------
# R에서 기본으로 제공되는 airquality 데이터 셋을 이용하기 위해서
#  - Temp(온도)에 영향을 미치는 변수를 알아보기 -
# airquality 데이터 셋 153개의 관측치와 6개의 변수로 구성되어 있으며
# New York의 대기에 관한 질을 측정한 데이터 셋이다.
# 주요 변수로는 Ozone(오존수치), Solar.R(태양광), Wind(바람),
# Temp(온도), Month(측정 월), Day(측정 날짜) 등이 있다.
#--------------------------------------------------------
# airquality 데이터 셋 로딩
library(datasets)
str(airquality)
# formula 생성
formula <-  Temp ~ Solar.R +  Wind + Ozone
# 분류모델 생성 : formula를 이용하여 분류모델 생성
air_ctree <- ctree(formula, data=airquality)
air_ctree
plot(air_ctree)
# <해설> 온도에 가장 큰 영향을 미치는 첫 번째 영향 변수는 오존수치(Ozone)이고,
# 두 번째 영향 변수는 바람(Wind)으로 나타난다.
#----------------<<실습2>>-------------------------------
#4개변수(Sepal Length,Sepal Width,Petal Length,Petal Width)
# 값에 따라서 꽃의 종류(Species)가 분류되는 분석 과정
#--------------------------------------------------------
#단계1 : 학습데이터와 검증데이터 샘플링
# set.seed(1234) # 메모리에 시드값 적용 - 동일값 생성
result <- sample(2, nrow(iris),replace=T, prob=c(0.7,0.3))
table(result) # 7:3비율 데이터 생성
train <- iris[result==1,]
test <- iris[result==2,]
# 단계2 : formula 생성
#  -> 형식) 변수 <- 종속변수 ~ 독립변수
formula <- Species ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width
#단계3 : 학습데이터 이용 분류모델 생성(ctree()함수 이용)
iris_ctree <- ctree(formula, data=train) # 학습데이터로 분류모델(tree) 생성
iris_ctree # Petal.Length,Petal.Width 중요변수
#단계4 : 분류모델 플로팅
# plot() 이용 - 의사결정 트리로 결과 플로팅
plot(iris_ctree, type="simple")
plot(iris_ctree) # 의사결정트리 해석
# 단계5 : 검정 데이터에 분류모델 적용(분류모델 검정)
iris_ctree2 <- ctree(formula, data=test)
iris_ctree2
# 1) 의사결정 트리 플로팅
plot(iris_ctree2)
# 2) 분류모델 예측
testpred <- predict(iris_ctree2, test) # 검정 데이터 적용
testpred
table(testpred,test$Species) # 3개 영역으로 분류
# <해설> 생성된 분류모델을 학습 데이터와 검정 데이터에 각각 적용하여
# 분석된 결과를 살펴보면 꽃의 종류를 분류하는데 있어서 가장 중요한 변수는
# Petal.Length(꽃잎길이)와 Petal.Width(꽃잎너비)로 모두 동일하게 나타났다.
# 위 결과는 표본에 의해서 추출된 데이터이기 때문에 실행결과는 다를 수 있다.
#----------------<<실습3>>-------------------------------
# 다이아몬드의 가격(price)에 미치는 영향변수 보기
# ggplot2 패키지에서 제공되는 diamonds 데이터 셋을 대상으로
# 반응변수를 price로 지정하고, 설명변수를 carat, cut, color,
# depth 지정하여 분류결과를 확인한다.
#--------------------------------------------------------
# 패키지 설치 및 로딩
install.packages("ggplot2")
library(ggplot2)
data(diamonds) # ggplot2에서 제공되는 데이터 셋 로딩
# 검정 데이터 만들기
# diamonds 데이터 셋의 전체 관측치는 53,860으로 구성되어 있기
# 때문에 그 중 50개만 표본으로 추출하여 검정 데이터를 생성한다.
# 관측치가 지나치게 많은 경우에는 효율성과 해석력이 떨질 수 있다.
t <- sample(1:nrow(diamonds), 50)
test <- diamonds[t, ] # 50개 관측치만 표본으로 추출
dim(test) # [1] 50 10
test # 검정 데이터
# formula 작성과 분류분석
# 반응변수를 price로 지정하고, 설명변수를 carat, cut, color, depth로
# 지정하여 분류분석을 위한 식으로 작성하고 분류분석을 수행 한다.
formula <- price ~ carat +  cut + color + depth # 분류분석 식 작성
diamonds_ctree <- ctree(formula, data=test) # 검정 데이터로 분류분석
diamonds_ctree
# <해설> diamonds 데이터 셋에서 추출된 50개의 검정 데이터를 대상으로
# 분류분석 한 결과에서 다이아몬드의 가격(prince)은 설명변수
# 4개 중에서 다이아몬드의 크기(carat)에 의해서 분류되는 것을 확인할 수 있다.
# 분류분석 결과 시각화
range(test$carat) # [1] 0.26 2.08
plot(diamonds_ctree)
#----------------<<실습4>>-------------------------------
# AdultUCI 데이터 셋을 이용한 분류분석
# AdultUCI 데이터 셋에 관한 설명
# arules패키지에서 제공되는 데이터 셋으로 성인을 대상으로
# 인구 소득에 관한 설문조사 데이터를 포함하고 있다.
# 전체 48,842개의 관측치와 15개 변수로 구성되어 있다.
# 주요 변수 : age(나이),workclass(직업:4개),education(교육수준:16개),
# occupation(직업:12개),race(인종:아시아계,백인),capital-gain(자본이득),
# capital-loss(자본손실),hours-per-week(주당 근무시간),
# native-country(국가),income(소득)
#-------------------------------------------------------------
install.packages("arules") # AdultUCI 데이터 셋 이용을 위한 패키지 설치
library(arules)
data("AdultUCI")
str(AdultUCI) # 'data.frame':	48842 obs. of  15 variables:
names(AdultUCI)
# 데이터 샘플링 - 10,000개 관측치 선택
set.seed(1234) # 메모리에 시드 값 적용
choice <- sample(1:nrow(AdultUCI), 10000)
choice
adult.df <-  AdultUCI[choice, ]
str(adult.df) # ' # 'data.frame':	10000 obs. of  15 variables:
# 변수 추출 및 데이터 프레임 생성
# (1) 변수 추출
capital<- adult.df$`capital-gain`
hours<- adult.df$`hours-per-week`
education <- adult.df$`education-num`
race <- adult.df$race
age <- adult.df$age
income <- adult.df$income
# (2) 데이터프레임 생성
adult_df <- data.frame(capital=capital, age=age, race=race, hours=hours, education=education, income=income)
str(adult_df) # 'data.frame':	10000 obs. of  6 variables:
# formula 생성 - 자본이득(capital)에 영향을 미치는 변수
formula <-  capital ~ income + education + hours + race + age
# ctree() : 분류모델 생성 및 예측
adult_ctree <- ctree(formula, data=adult_df)
adult_ctree # 가장 큰 영향을 미치는 변수 - income, education
# ※ 주의 : random으로 실행되기 때문에 실행결과는 다를 수 있다.
# 분류모델 플로팅
plot(adult_ctree)
# <해설> 자본이득(capital)에 가장 큰 영향을 미치는 변수는 income이고,
# 두 번째는 education 변수이다.
#--------------------------------------------------------------------
#<연습문제1> 분류모델 예측
#--------------------------------------------------------------------
#################################################
# rpart 패키지 적용 분류분석
# - rpart 설치 -> 32개 트리까지 생성
#################################################
# 1. rpart()함수 간단 실습
install.packages("rpart")
library(rpart)
X11() # 별도창
formula <- Species ~ .
iris.df <- rpart(formula, data=iris)
iris.df
plot(iris.df ) # 트리 프레임 보임
text(iris.df, use.n=T, cex=0.6) # 텍스트 추가
post(iris.df, file="")
# <해석>
# iris의 Species(꽃의 종류)변수를 분류하는 가장 중요한 변수는
# Petal.Length와 Petal.Width로 나타난다.
# 2. rpart() 응용 실습
#  weather.csv를 weather로 읽어서 RainTomorrow가 y변수, Data, RainTody를
#  제외한 나머지 변수가 x변수가 되도록 하여 decision tree를 작성
########################## <weather set> ###########################
# Date(측정날짜) MinTemp(최저기온) MaxTemp(최대기온) Rainfall(강수량)
# Sunshine(햇빛)  WindGustDir(돌풍 방향) WindGustSpeed(돌풍 속도)
# WindDir(바람방향) WindSpeed(바람속도) Humidity(습도) Pressure(기압)
# Cloud(구름) Temp(온도) RainToday(오늘 비 여부) RainTomorrow(내일 비 여부)
#################################################################
# 날씨에 따라서 비가 내릴지의 여부를 기록한 데이터이다.
# 이 데이터를 분석하면 어떤 날씨 조건에 비가 내릴지 또는 내리지 않을지에
# 대한 판단 기준을 분석할 수 있다.
#################################################################
# 1) 데이터 가져오기
# c:/Rwork/Part-IV/weather.csv 파일 선택
weather = read.csv("c:/Rwork/Part-IV/weather.csv", header=TRUE)
# 2) 데이터 특성 보기
str(weather) # data.frame':  366 obs. of  15 variables:
names(weather) # 15개 변수명
head(weather)
# 3) 분류분석 - 의사결정트리 생성
weather.df <- rpart(RainTomorrow~.,
data=weather[, c(-1,-14)], cp=0.01)
weather.df
# cp 속성 값을 높이면 가지 수가 적어지고, 낮추면 가지 수가 많아진다.
# cp 기본값은 0.01
# 4) 분류분석 시각화
X11()
plot(weather.df) # 트리 프레임 보임
text(weather.df, use.n=T, cex=0.7) # 텍스트 추가
# post(weather.df, file="") # 타원제공 - rpart 패키지 제공
# <해설>
# 분기조건이 참이면 왼쪽으로 분류되고, 거짓이면 오른쪽으로 분류된다.
#--------------------------------------------------------------------
# <연습문제2> 분류모델 예측
#--------------------------------------------------------------------
