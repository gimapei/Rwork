filedata <- read.csv('c:/Rwork/Part-I/test.csv', header=T)
filedata
filedata <- read.csv('c:/Rwork/Part-I/test.csv', header=T) # 기본폴더에서 가져올때는 그냥 파일명만 명시해도 된다.
filedata
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_77')
library(rJava) # 로딩
library(KoNLP)      # 한글 단어 처리 rjava 영향
library(KoNLP)      # 한글 단어 처리 rjava 영향
library(tm)         # 텍스트 마이닝(전처리용)
library(wordcloud)  # 단어 구름 생성
library(tm)         # 텍스트 마이닝(전처리용)
library(wordcloud)  # 단어 구름 생성
extractNoun("나는 대한민국 국민이다. 국민은 ")
facebook <- file("C:/Rwork/Part-II/facebook_bigdata.txt", encoding="UTF-8")
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(facebook_data) # chr [1:76]
facebook_corpus <- Corpus(VectorSource(facebook_data))
facebook_corpus
# 76개 자료집 보기 - 포함된 문자 수 제공
inspect(facebook_corpus)
# 3. 분석 대상 자료집을 대상으로 NA 처리(공백)
facebook_corpus[is.na(facebook_corpus)]   <- " "
facebook_corpus
# 4. 세종 사전 사용 및 단어 추가
useSejongDic() # 세종 사전 불러오기
# 세종 사전에 없는 단어 추가
mergeUserDic(data.frame(c("R 프로그래밍","페이스북","소셜네트워크"), c("ncn")))
# ncn -명사지시코드
# 5. 단어추출 사용자 함수 정의
# (1) 사용자 정의 함수 실행 순서 : 문자변환 -> 명사 단어추출 -> 공백으로 합침
exNouns <- function(x) {
paste(extractNoun(as.character(x)), collapse=" ")
}
# (2) exNouns 함수 이용 단어 추출
# 형식) sapply(적용 데이터, 적용함수)
facebook_nouns <- sapply(facebook_corpus, exNouns)
# (3) 단어 추출 결과
#class(facebook_nouns) # [1] "character" -> Vector 타입
facebook_nouns[1] # 단어만 추출된 첫 줄 보기
# 6. 데이터 전처리
# (1) 추출된 단어 이용하여 자료집 생성
myCorputfacebook <- Corpus(VectorSource(facebook_nouns))
# (2) 데이터 전처리
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myCorputfacebook <-tm_map(myCorputfacebook, removeWords, stopwords('english')) # 불용어제거
# (3) 전처리 결과 확인
inspect(myCorputfacebook[1:5]) # 데이터 전처리 결과 확인
# 7. 단어 선별(단어 길이 2개 이상)
# (1) 자료집 -> 일반문서 변경
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
# (2) TermDocumentMatrix() : 단어 선별(단어길이 2개 이상인 단어 선별 -> matrix 변경)
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myCorputfacebook_txt
# (3) matrix -> data.frame 변경(빈도분석을 위해서)
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 876  76
# 8. 단어 빈도수 구하기(행 단위 합계 -> 내림차순 정렬)
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적용
head(facebook_corpus)
head(facebook_corpus)
inspect(facebook_corpus)
useSejongDic() # 세종 사전 불러오기
mergeUserDic(data.frame(c("R 프로그래밍","페이스북","소셜네트워크"), c("ncn")))
wordcloud(c("한국","일본"))
wordcloud(c("한국","일본"), c(10,5))
#---------------<불필요한 단어 제거 시작 :  6 ~ 9단계 수행>-------------------
# 6. 데이터 전처리 - "사용", "하기" 단어 제거
myCorputfacebook <- tm_map(myCorputfacebook, removePunctuation) # 문장부호 제거
myCorputfacebook <- tm_map(myCorputfacebook, removeNumbers) # 수치 제거
myCorputfacebook <- tm_map(myCorputfacebook, tolower) # 소문자 변경
myStopwords = c(stopwords('english'), "사용", "하기");
myCorputfacebook = tm_map(myCorputfacebook, removeWords, myStopwords);
# 7. 단어 선별(단어 길이 2개 이상)
myCorputfacebook_txt <- tm_map(myCorputfacebook, PlainTextDocument)
myCorputfacebook_txt <- TermDocumentMatrix(myCorputfacebook_txt, control=list(wordLengths=c(2,Inf)))
myTermfacebook.df <- as.data.frame(as.matrix(myCorputfacebook_txt))
dim(myTermfacebook.df) # [1] 876  76
# 8. 단어 빈도수 구하기
wordResult <- sort(rowSums(myTermfacebook.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
#빅데이터     분석     처리     수집     결과
#21       19       16       15       13
# 9. 단어 구름(wordcloud) 생성  생성 - 디자인 적용 전
myName <- names(wordResult) # 단어 이름 추출(빈도수 이름)
wordcloud(myName, wordResult) # 단어구름 시각화
#myName
#------------------<불필요한 단어 제거 종료 >-----------------------
# 10. 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# (1) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
# (2) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# (3) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
#wordcloud(단어, 빈도수, 5:1비율 크기,최소빈도수,랜덤순서,랜덤색상, 회전비율, 색상(파렛트),컬러,글꼴 )
# 11. 차트 시각화
#(1) 상위 10개 토픽추출
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
# (2) 파일 차트 생성
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
# (3) 빈도수 백분율 적용
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
names(topWord)
# (4) 단어와 백분율 하나로 합친다.
lab <- paste(names(topWord), "\n", pct, "%")
# (5) 파이차트에 단어와 백분율을 레이블로 적용
pie(topWord, main="SNS 빅데이터 관련 토픽분석", col=rainbow(10), cex=0.8, labels=lab)
gul <- file("C:/Rwork/Part-II/abstractClean.txt", encoding="UTF-8")
gul_data <- readLines(gul) # 줄 단위 데이터 생성
head(gul_data) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(gul_data) # chr [1:76]
gul_corpus <- Corpus(VectorSource(gul_data))
gul_corpus
head(gul_corpus)
inspect(gul_corpus)
gul_corpus[is.na(gul_corpus)]   <- " "
gul_corpus
useSejongDic() # 세종 사전 불러오기
exNouns <- function(x) {
paste(extractNoun(as.character(x)), collapse=" ")
}
gul_nouns <- sapply(gul_corpus, exNouns)
gul_nouns[1] # 단어만 추출된 첫 줄 보기
myCorputgul <- Corpus(VectorSource(gul_nouns))
myCorputgul <- tm_map(myCorputgul, removePunctuation) # 문장부호 제거
myCorputgul <- tm_map(myCorputgul, removeNumbers) # 수치 제거
myCorputgul <- tm_map(myCorputgul, tolower) # 소문자 변경
myCorputgul <-tm_map(myCorputgul, removeWords, stopwords('english')) # 불용어제거
inspect(myCorputgul[1:5]) # 데이터 전처리 결과 확인
myCorputgul_txt <- tm_map(myCorputgul, PlainTextDocument)
myCorputgul_txt <- TermDocumentMatrix(myCorputgul_txt, control=list(wordLengths=c(2,Inf)))
myCorputgul_txt
myTermgul.df <- as.data.frame(as.matrix(myCorputgul_txt))
dim(myTermgul.df) # [1] 876  76
wordResult <- sort(rowSums(myTermgul.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적용
myStopwords = c(stopwords('english'), "연구", "제");
myStopwords = c(stopwords('english'), "연구", "제품");
myCorputgul = tm_map(myCorputgul, removeWords, myStopwords);
inspect(myCorputgul[1:5]) # 데이터 전처리 결과 확인
myCorputgul_txt <- tm_map(myCorputgul, PlainTextDocument)
# (2) TermDocumentMatrix() : 단어 선별(단어길이 2개 이상인 단어 선별 -> matrix 변경)
myCorputgul_txt <- TermDocumentMatrix(myCorputgul_txt, control=list(wordLengths=c(2,Inf)))
myCorputgul_txt
# (3) matrix -> data.frame 변경(빈도분석을 위해서)
myTermgul.df <- as.data.frame(as.matrix(myCorputgul_txt))
dim(myTermgul.df) # [1] 876  76
# 8. 단어 빈도수 구하기(행 단위 합계 -> 내림차순 정렬)
wordResult <- sort(rowSums(myTermgul.df), decreasing=TRUE) # 빈도수로 내림차순 정렬
wordResult[1:10]
#빅데이터     사용     분석     처리     수집
#      21       20       19       16       15
# 9. wordcloud 생성 (디자인 적용전)
myName <- names(wordResult) # 단어 이름 생성 -> 빈도수의 이름
wordcloud(myName, wordResult) # 단어구름 적용
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df) # word, freq 변수
# (2) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# 폰트 설정세팅 : "맑은 고딕", "서울남산체 B"
windowsFonts(malgun=windowsFont("맑은 고딕"))  #windows
# (3) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
x11( ) # 별도의 창을 띄우는 함수
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
#wordcloud(단어, 빈도수, 5:1비율 크기,최소빈도수,랜덤순서,랜덤색상, 회전비율, 색상(파렛트),컬러,글꼴 )
# 11. 차트 시각화
#(1) 상위 10개 토픽추출
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
# (2) 파일 차트 생성
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
# (3) 빈도수 백분율 적용
pct <- round(topWord/sum(topWord)*100, 1) # 백분율
names(topWord)
# (4) 단어와 백분율 하나로 합친다.
lab <- paste(names(topWord), "\n", pct, "%")
# (5) 파이차트에 단어와 백분율을 레이블로 적용
pie(topWord, main="SNS 빅데이터 관련 토픽분석", col=rainbow(10), cex=0.8, labels=lab)
topWord <- head(sort(wordResult, decreasing=T), 10) # 상위 10개 토픽추출
# (2) 파일 차트 생성
pie(topWord, col=rainbow(10), radius=1) # 파이 차트-무지개색, 원크기
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=3, random.order=F,
rot.per=.1, colors=pal, family="malgun")
#wordcloud(단어, 빈도수, 5:1비율 크기,최소빈도수,랜덤순서,랜덤색상, 회전비율, 색상(파렛트),컬러,글꼴 )
# 11. 차트 시각화
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_77')
library(rJava) # 아래와 같은 Error 발생 시 Sys.setenv()함수로 java 경로 지정
install.packages("KoNLP")
install.packages("KoNLP")
library(KoNLP) # rJava 라이브러리가 필요함
marketing <- file("C:/Rwork/Part-II/marketing.txt", encoding="UTF-8")
marketing2 <- readLines(marketing) # 줄 단위 데이터 생성
close(marketing)
head(marketing2) # 앞부분 6줄 보기 - 줄 단위 문장 확인
lword <- Map(extractNoun, marketing2)
lword
lword
lword <- Map(extractNoun, marketing2)
lword
length(lword) # [1] 472
lword <- unique(lword) # 중복제거1(전체 대상)
length(lword) # [1] 353(19개 제거)
length(lword) # [1] 352(1개 제거)
length(lword) # [1] 472
lword <- unique(lword) # 중복제거1(전체 대상)
length(lword) # [1] 353(19개 제거)
lword <- sapply(lword, unique) # 중복제거2(줄 단위 대상)
length(lword) # [1] 352(1개 제거)
str(lword) # List of 353
lword # 추출 단어 확인
filter1 <- function(x){
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
filter2 <- function(x){
Filter(filter1, x)
}
lword <- sapply(lword, filter2)
lword # 추출 단어 확인(길이 1개 단어 삭제됨)
install.packages("arules")
library(arules)
wordtran <- as(lword, "transactions") # lword에 중복데이터가 있으면 error발생
wordtran
wordtable <- crossTable(wordtran) # 교차표 작성
wordtable
tranrules <- apriori(wordtran, parameter=list(supp=0.25, conf=0.05))
inspect(tranrules) # 연관규칙 생성 결과(59개) 보기
rules <- labels(tranrules, ruleSep=" ")
class(rules)
#[1] "character"
rules <- sapply(rules, strsplit, " ",  USE.NAMES=F)
rulemat <- do.call("rbind", rules)
# sapply(), do.call() # base 패키지
rulemat
class(rulemat)
#[1] "matrix"
install.packages("igraph") # graph.edgelist(), plot.igraph(), closeness() 함수 제공
library(igraph)
library(igraph)
library(igraph)
install.packages("igraph") # graph.edgelist(), plot.igraph(), closeness() 함수 제공
library(igraph)
ruleg <- graph.edgelist(rulemat[c(12:59),], directed=F) # [1,]~[11,] "{}" 제외
ruleg
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
# 정점(타원) 레이블 속성
closen <- closeness(ruleg) # edgelist 대상 단어 근접중심성 생성
#closen <- closen[c(2:8)] # {} 항목제거
closen <- closen[c(1:10)] # 상위 1~10 단어 근접중심성 보기
plot(closen, col="red",xaxt="n", lty="solid", type="b", xlab="단어", ylab="closeness")
points(closen, pch=16, col="navy")
axis(1, seq(1, length(closen)), V(ruleg)$name[c(1:10)], cex=5)
# 중심성 : 노드(node)의 상대적 중요성을 나타내는 척도이다.
# plot, points(), axis() : graphics 패키지(기존 설치됨)
#---------------------------------------------------------------
#<연관어분석 관련 연습문제>
#---------------------------------------------------------------
closen <- closeness(ruleg) # edgelist 대상 단어 근접중심성 생성
#closen <- closen[c(2:8)] # {} 항목제거
closen <- closen[c(1:10)] # 상위 1~10 단어 근접중심성 보기
plot(closen, col="red",xaxt="n", lty="solid", type="b", xlab="단어", ylab="closeness")
points(closen, pch=16, col="navy")
axis(1, seq(1, length(closen)), V(ruleg)$name[c(1:10)], cex=5)
data<-read.csv(file.choose()) # file.choose() 파일 선택
head(data,2)
dim(data) # 100   2
str(data) # 변수명 : company, review <- 고객 인터뷰 내용
setwd("C:/Rwork/Part-II")
posDic <- readLines("posDic.txt")
negDic <- readLines("negDic.txt")
length(posDic) # 2006
length(negDic) # 4783
# (2) 긍정어/부정어 단어 추가
posDic.final <-c(posDic, 'victor')
negDic.final <-c(negDic, 'vanquished')
# 마지막에 단어 추가
posDic.final;
negDic.final
library(plyr) # laply()함수 제공
library(stringr) # str_split()함수 제공
# (2) 감성분석을 위한 함수 정의
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
# 4) 감성 분석 : 두번째 변수(review) 전체 레코드 대상 감성분석
result<-sentimental(data[,2], posDic.final, negDic.final)
result
names(result) # "score" "text"
dim(result) # 100   2
result$text
result$score # 100 줄 단위로 긍정어/부정어 사전을 적용한 점수 합계
# score값을 대상으로 color 칼럼 추가
result$color[result$score < 0] <- "red"
result$color[result$score >=1] <- "blue"
result$color[result$score ==0] <- "green"
# 감성분석 결과 차트보기
plot(result$score, col=result$color) # 산포도 색생 적용
barplot(result$score, col=result$color, main ="감성분석 결과화면") # 막대차트
# 5) 단어의 긍정/부정 분석
table(result$color)
# (1) 감성분석 빈도수
# (2) score 칼럼 리코딩
result$remark[result$score < 0] <- "부정"
result$remark[result$score >=1] <- "긍정"
result$remark[result$score ==0] <- "중립"
sentiment_result<- table(result$remark)
sentiment_result
# (3) 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
setwd("C:/Rwork/Part-II")
dataset <- read.csv("dataset.csv", header=TRUE) # 헤더가 있는 경우
print(dataset)
View(dataset) # 뷰어창 출력
head(dataset)
tail(dataset)
# 2) 데이터 셋 구조보기
names(dataset) # 변수명(컬럼)
attributes(dataset) # names(), class, row.names
attributes(dataset) # names(), class, row.names
str(dataset) # 데이터 구조보기
dataset$age
dataset$resident
length(dataset$age) # data 수-300개
x <- dataset$gender # 조회결과 변수 저장
y <- dataset$price
x;y
plot(dataset$price) # 산점도 형태 전반적인 가격분포 보기
dataset["gender"]
dataset["price"]
dataset[2] # 두번째 컬럼
dataset[6] # 여섯번째 컬럼
dataset[3,] # 3번째 관찰치(행) 전체
dataset[,3] # 3번째 변수(열) 전체
# dataset 데이터 중 변수를 2개 이상 조회하는 경우
dataset[c("job","price")]
dataset[c(2,6)]
dataset[c(1,2,3)]
dataset[c(1:3)]
dataset[c(2,4:6,3,1)]
dataset[c("job","price")]
dataset[1:50,c("job","price")]
dataset[1:10,c("job","price")]
dataset[c(1,2,3)]
dataset[c(1:3)]
dataset[c(2,4:6,3,1)]
dataset[5,c(1,2,3)]
dataset[5,c(1:3)]
dataset[5,c(2,4:6,3,1)]
summary(dataset$price)
sum(dataset$price) # NA 출력
sum(dataset$price, na.rm=T) # 2362.9
price2 <- na.omit(dataset$price)
sum(price2) # 2362.9
length(price2) # 270 -> 30개 제거
summary(dataset$price)
sum(dataset$price) # NA 출력
dataset$price <- str_replace_na(dataset$price)
sum(dataset$price) # NA 출력
dataset$price <- as.numeric(dataset$price)
dataset$price <- as.numeric(dataset$price)
sum(dataset$price) # NA 출력
dataset$price <- str_replace_na(dataset$price)
dataset$price <- as.numeric(dataset$price)
summary(dataset$price)
sum(dataset$price) # NA 출력
sum(dataset$price, na.rm=T) # 2362.9
price2 <- na.omit(dataset$price)
sum(price2) # 2362.9
length(price2) # 270 -> 30개 제거
# 3. 극단치 발견과 정제
gender <- dataset$gender
gender
summary(dataset$price)
gender <- dataset$gender
gender
hist(gender) # 히스토그램
table(gender) # 빈도수
pie(table(gender)) # 파이 차트
data <- subset(dataset,dataset$gender==1 | dataset$gender==2)
data # gender변수 데이터 정제
length(data$gender) # 297개 - 3개 정제됨
pie(table(data$gender))
data <- subset(dataset,dataset$gender==1 | dataset$gender==2)
data # gender변수 데이터 정제
length(data$gender) # 297개 - 3개 정제됨
pie(table(data$gender))
dataset$price # 세부데이터 보기
length(dataset$price) #300개(NA포함)
plot(dataset$price) # 산점도
summary(dataset$price) # 범위확인
data <- subset(dataset, dataset$price >= 2 & dataset$price <= 8)
length(data$price)
stem(data$price) # 줄기와 잎 도표보기
data$survey
survey <- data$survey
csurvey <- 6-survey # 역코딩
csurvey
survey  # 역코딩 결과와 비교
data$survey <- csurvey # survery 수정
head(data) # survey 결과 확인
data$survey
survey <- data$survey
csurvey <- 6-survey # 역코딩
csurvey
survey  # 역코딩 결과와 비교
data$survey <- csurvey # survery 수정
head(data) # survey 결과 확인
head(survey)
head(survey)
data$survey
survey <- data$survey
head(survey)
csurvey <- 6-survey # 역코딩
csurvey
head(csurvey)
survey  # 역코딩 결과와 비교
data$survey <- csurvey # survery 수정
head(data) # survey 결과 확인
# 5. 코딩변경 - 나이 기준 변수 리코딩
data$age2[data$age <= 30] <-"청년층"
data$age2[data$age > 30 & data$age <=45] <-"중년층"
data$age2[data$age > 45] <-"장년층"
#6.파생변수 생성 - 기존 데이터로 새로운 변수 생성
data$resident2[data$resident == 1] <-"특별시"
data$resident2[data$resident >=2 & data$resident <=4] <-"광역시"
head(data) # data 테이블 전체 - age2 컬럼 생성
data$resident2[data$resident == 5] <-"시구군"
head(data[c("resident","resident2")]) # 2개만 지정
dim(data$age2)
data$age2[data$age <= 30] <-"청년층"
dim(data$age2)
data
dim(data$age)
dim(data$age)
summary(data$age)
data$age2[data$age <= 30] <-"청년층"
data$age2[data$age > 30 & data$age <=45] <-"중년층"
data$age2[data$age > 45] <-"장년층"
data$age2
head(c(age, age2))
head(c(age, age2))
head(c(data$age, data$age2))
head(data[c('age','age2')]
head(data[c('age','age2')])
head(data[c('age','age2')])
head(data[c('age','age2')])
data$resident2[data$resident == 1] <-"특별시"
data$resident2[data$resident >=2 & data$resident <=4] <-"광역시"
data$resident2[data$resident == 5] <-"시구군"
head(data) # data 테이블 전체 - age2 컬럼 생성
head(data[c("resident","resident2")]) # 2개만 지정
# 6.파생변수 생성 - 기존 데이터로 새로운 변수 생성
data$resident2[data$resident == 1] <-"특별시"
data$resident2[data$resident >=2 & data$resident <=4] <-"광역시"
data$resident2[data$resident == 5] <-"시구군"
head(data) # data 테이블 전체 - age2 컬럼 생성
head(data[c("resident","resident2")]) # 2개만 지정
getwd()
setwd("c:/Rwork/Part-II")
write.csv(data,"cleanData.csv", quote=F, row.names=F)
# (2) 저장된 파일 불러오기/확인
data <- read.csv("cleanData.csv", header=TRUE)
data
length(data$age) # 길이 확인
choice <- sample(1:nrow(data),30) # 30개 무작위 추출
choice # 추출값 : 행 번호
choice2 <- sample(nrow(data), 30) #sample과 동일
choice2
choice3 <- sample(50:nrow(data), 30) # 50~end
choice3
choice4 <- sample(c(50:100), 30) # 50~100
choice4
#다양한 범위를 지정해서 무작위 셈플링
choice5 <- sample(c(10:50, 70:150, 160:190),30)
choice5
# 마지막 행수 직접 입력
choicePrice <- sample(1:234,30)
choicePrice # 셈플링 결과
# 특정 변수 대상 셈플링 불가
